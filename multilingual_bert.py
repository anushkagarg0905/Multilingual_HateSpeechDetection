# -*- coding: utf-8 -*-
"""Multilingual_BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W3EVCuDna_tNrk6BisYyJg5IjpJiGWbx

#Multilingual Hate Speech Detection using Transformers
This notebook implements a multilingual hate speech detection system using transformers. The goal is to classify text as hateful or non-hateful across hindi, english and hinglish(mix) and compare the performance of two multilingual models: mBERT and XLM-RoBERTa.

Installing required libraries for training and evaluation:
"""

!pip install -q transformers datasets scikit-learn torch accelerate

import pandas as pd
import numpy as np
import torch

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)

"""Loading the dataset and retain only relevant columns:"""

df = pd.read_csv("combined_hate_speech_dataset.csv")

df = df[['text', 'hate_label', 'profanity_score', 'language']]
df = df.dropna()

df.head()

df['hate_label'] = df['hate_label'].astype(int)

"""Spliting the dataset for training and testing:"""

train_df, test_df = train_test_split(
    df,
    test_size=0.2,
    stratify=df['hate_label'],
    random_state=42
)

print("Train size:", len(train_df))
print("Test size:", len(test_df))

"""Performing Text Tokanization: Converting raw text into token that the model can understand using padding(sequences of token in the batch have the same length) and trucation(reduce length if beyond the limit)."""

def tokenize_data(tokenizer, texts):
    return tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=64
    )

"""Create a custom PyTorch Dataset to efficiently pass tokenized inputs and labels to the model."""

class HateDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

"""Evaluation:"""

def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)

    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average='binary'
    )
    acc = accuracy_score(labels, preds)

    return {
        "accuracy": acc,
        "precision": precision,
        "recall": recall,
        "f1": f1
    }

"""Model Training: Used for both mBERT and XLM-RoBERTa"""

def train_model(model_name, train_df, test_df):
    print(f"\nTraining {model_name}...\n")

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2
    )

    train_enc = tokenize_data(tokenizer, train_df['text'].tolist())
    test_enc = tokenize_data(tokenizer, test_df['text'].tolist())

    train_dataset = HateDataset(train_enc, train_df['hate_label'].tolist())
    test_dataset = HateDataset(test_enc, test_df['hate_label'].tolist())

    args = TrainingArguments(
        output_dir=f"./results_{model_name}",
        do_eval=True,
        save_strategy="no",
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        num_train_epochs=1,
        learning_rate=2e-5,
        logging_steps=50,
        report_to="none"
    )

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        compute_metrics=compute_metrics
    )

    trainer.train()
    metrics = trainer.evaluate()

    return trainer, tokenizer, metrics

"""Training mBERT Model: Fine-tune the multilingual BERT (mBERT) model on the training dataset and evaluate"""

mbert_trainer, mbert_tokenizer, mbert_metrics = train_model(
    "bert-base-multilingual-cased",
    train_df,
    test_df
)

mbert_metrics

"""Training XLM-RoBERTa Model: Fine-tune the XLM-RoBERTa model on the training dataset and evaluate"""

xlmr_trainer, xlmr_tokenizer, xlmr_metrics = train_model(
    "xlm-roberta-base",
    train_df,
    test_df
)

xlmr_metrics

"""Model Performance Comparision"""

comparison = pd.DataFrame({
    "Model": ["mBERT", "XLM-RoBERTa"],
    "Accuracy": [mbert_metrics['eval_accuracy'], xlmr_metrics['eval_accuracy']],
    "F1 Score": [mbert_metrics['eval_f1'], xlmr_metrics['eval_f1']]
})

comparison

"""Language-wise Error Analysis: Analyzing how model performance varies across languages"""

def language_wise_errors(trainer, tokenizer, df):
    enc = tokenize_data(tokenizer, df['text'].tolist())
    dataset = HateDataset(enc, df['hate_label'].tolist())

    preds = trainer.predict(dataset)
    y_pred = np.argmax(preds.predictions, axis=1)

    df_temp = df.copy()
    df_temp['predicted'] = y_pred
    df_temp['error'] = df_temp['hate_label'] != df_temp['predicted']

    return df_temp.groupby('language')['error'].mean().sort_values(ascending=False)

language_wise_errors(mbert_trainer, mbert_tokenizer, test_df)

language_wise_errors(xlmr_trainer, xlmr_tokenizer, test_df)

"""Confusion Matrix Analysis: Helps understanding False Positives and False Negatives"""

def show_confusion(trainer, tokenizer, df):
    enc = tokenize_data(tokenizer, df['text'].tolist())
    dataset = HateDataset(enc, df['hate_label'].tolist())

    preds = trainer.predict(dataset)
    y_pred = np.argmax(preds.predictions, axis=1)

    return confusion_matrix(df['hate_label'], y_pred)

show_confusion(xlmr_trainer, xlmr_tokenizer, test_df)

"""Importing libraries used for Visulaizations"""

import matplotlib.pyplot as plt
import seaborn as sns

"""Model Performance Comparison Bar Chart"""

models = ['mBERT', 'XLM-RoBERTa']
accuracy = [mbert_metrics['eval_accuracy'], xlmr_metrics['eval_accuracy']]
f1 = [mbert_metrics['eval_f1'], xlmr_metrics['eval_f1']]

x = np.arange(len(models))
width = 0.35

plt.figure(figsize=(7,5))
plt.bar(x - width/2, accuracy, width, label='Accuracy')
plt.bar(x + width/2, f1, width, label='F1 Score')

plt.xticks(x, models)
plt.ylabel('Score')
plt.title('Model Performance Comparison')
plt.legend()
plt.show()

"""Detailed Metrics Visualization (Precision vs Recall)"""

metrics_df = pd.DataFrame({
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1'],
    'mBERT': [
        mbert_metrics['eval_accuracy'],
        mbert_metrics['eval_precision'],
        mbert_metrics['eval_recall'],
        mbert_metrics['eval_f1']
    ],
    'XLM-RoBERTa': [
        xlmr_metrics['eval_accuracy'],
        xlmr_metrics['eval_precision'],
        xlmr_metrics['eval_recall'],
        xlmr_metrics['eval_f1']
    ]
})

metrics_df.set_index('Metric').plot(
    kind='bar',
    figsize=(8,5)
)

plt.title('Detailed Metric Comparison')
plt.ylabel('Score')
plt.xticks(rotation=0)
plt.show()

"""Language-Wise Error Rate Visualization with Error Caluculation"""

def get_language_errors(trainer, tokenizer, df):
    enc = tokenize_data(tokenizer, df['text'].tolist())
    dataset = HateDataset(enc, df['hate_label'].tolist())

    preds = trainer.predict(dataset)
    y_pred = np.argmax(preds.predictions, axis=1)

    temp = df.copy()
    temp['pred'] = y_pred
    temp['error'] = temp['pred'] != temp['hate_label']

    return temp.groupby('language')['error'].mean().sort_values(ascending=False)

mbert_lang_error = get_language_errors(
    mbert_trainer, mbert_tokenizer, test_df
)

xlmr_lang_error = get_language_errors(
    xlmr_trainer, xlmr_tokenizer, test_df
)

plt.figure(figsize=(10,4))
mbert_lang_error.plot(kind='bar')
plt.title('mBERT Language-wise Error Rate')
plt.ylabel('Error Rate')
plt.xlabel('Language')
plt.show()

plt.figure(figsize=(10,4))
xlmr_lang_error.plot(kind='bar', color='orange')
plt.title('XLM-RoBERTa Language-wise Error Rate')
plt.ylabel('Error Rate')
plt.xlabel('Language')
plt.show()